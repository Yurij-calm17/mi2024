{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**Тема 4. Заняття 7. Основи використання методів глибокого навчання для аналізу даних**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Мета та завдання заняття\n",
    "- **Мета**: Ознайомити слухачів з основними методами глибокого навчання, навчити застосовувати їх для розв’язання базових задач регресії, класифікації та розпізнавання зображень.\n",
    "- **Завдання**:\n",
    "  1. Надати огляд основних концепцій та архітектур глибокого навчання.\n",
    "  2. Розглянути процес побудови та навчання глибокої нейронної мережі в середовищі TensorFlow.\n",
    "  3. Відпрацювати практичні навички застосування глибокого навчання для задач:\n",
    "     - Регресії (приклад: прогнозування певної кількісної змінної).\n",
    "     - Класифікації (приклад: класифікація ірисів чи інший відкритий набір даних).\n",
    "     - Розпізнавання рукописних цифр (MNIST).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Структура заняття\n",
    "\n",
    "1. **Організаційний момент (5 хв.)**  \n",
    "   - Перевірка присутності.\n",
    "   - Ознайомлення з планом і цілями заняття.\n",
    "\n",
    "2. **Актуалізація знань (10 хв.)**  \n",
    "   - Коротке повторення базових понять машинного навчання:\n",
    "     - Що таке нейронна мережа.\n",
    "     - Чим відрізняється глибоке навчання від звичайного (менш глибокого) машинного навчання.\n",
    "     - Приклади успішного застосування глибоких нейронних мереж.\n",
    "\n",
    "3. **Теоретичний блок (20 хв.)**\n",
    "   - **3.1. Огляд основних методів глибокого навчання**  \n",
    "     - Перцептрон, багатошарова нейронна мережа (Multilayer Perceptron, MLP).  \n",
    "     - Згорткові нейронні мережі (CNN) для аналізу зображень.  \n",
    "     - Рекурентні нейронні мережі (RNN) та LSTM/GRU для послідовних даних (текст, тимчасові ряди).  \n",
    "     - Трансформери (оглядово).  \n",
    "   - **3.2. Основи використання методів глибокого навчання для аналізу даних**  \n",
    "     - Структура проєкту глибокого навчання:  \n",
    "       1. Завантаження та попередня обробка даних.  \n",
    "       2. Розподіл на тренувальний/валідаційний/тестовий набори.  \n",
    "       3. Побудова моделі (визначення архітектури).  \n",
    "       4. Вибір функції втрат, метрик та оптимізатора.  \n",
    "       5. Навчання, валідація та оцінка результатів.  \n",
    "       6. Тонке налаштування (Hyperparameter Tuning).  \n",
    "     - Ключові поняття (епохи, батчі, функції активації, регуляризація).\n",
    "\n",
    "4. **Практичний блок (45–60 хв.)**\n",
    "   1. **Задача регресії** (приклад на базі вбудованого датасету або відкритого набору з `tensorflow.keras.datasets` чи іншого джерела).  \n",
    "      - Приклад: Прогнозування цін на нерухомість (якщо використовувати класичний датасет Boston Housing, що зараз доступний здебільшого поза пакетом keras, або інший схожий набір).\n",
    "   2. **Задача класифікації** (приклад класифікації ірисів або іншого датасету).  \n",
    "      - Приклад: Вбудований датасет Iris (не входить до `tf.keras.datasets`, але можна завантажити з інших публічних джерел), або `fashion_mnist`, `reuters`, тощо.\n",
    "   3. **Задача розпізнавання рукописних цифр** (MNIST).  \n",
    "      - Використання вбудованого `tf.keras.datasets.mnist`.\n",
    "\n",
    "   У кожному прикладі розглянути:\n",
    "   - Як підготувати дані.\n",
    "   - Як побудувати нейронну мережу (MLP або CNN для зображень).\n",
    "   - Як налаштувати параметри навчання.\n",
    "   - Як інтерпретувати результати (точність, втрата, візуалізація).\n",
    "\n",
    "5. **Підсумкове обговорення (10 хв.)**\n",
    "   - Питання та відповіді.\n",
    "   - Коротка рефлексія: з якими труднощами зіткнулися, що вдалося найкраще.\n",
    "\n",
    "6. **Завдання для самостійної роботи**\n",
    "   - Самостійно обрати інший набір даних та спробувати побудувати регресійну або класифікаційну модель.  \n",
    "   - Погратися з різними параметрами (кількість шарів, кількість нейронів, функції активації) та оцінити вплив на якість прогнозу.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Приклади коду\n",
    "\n",
    "Нижче наведено спрощені приклади використання `TensorFlow`/`Keras` для вирішення задач регресії, класифікації та розпізнавання цифр MNIST. Код можна адаптувати під конкретне завдання та набір даних.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.1. Завдання регресії (Приклад: Boston Housing)\n",
    "\n",
    "> **Примітка**: Набір даних Boston Housing було вилучено з `tf.keras.datasets` з етичних міркувань. Його можна завантажити з інших джерел або використати інший набір даних для регресії (наприклад, California Housing чи власний CSV). Нижче – умовний приклад, якщо дані вже доступні.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Завантаження та підготовка даних (приклад Boston, якщо доступний)\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Нормалізація\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Розподіл на тренувальні та тестові дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# 2. Створення моделі\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Для регресії вихід 1\n",
    "\n",
    "# 3. Компіляція моделі\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mse', \n",
    "              metrics=['mae'])\n",
    "\n",
    "# 4. Навчання\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=50,\n",
    "                    batch_size=16,\n",
    "                    verbose=1)\n",
    "\n",
    "# 5. Оцінка\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# 6. Прогноз\n",
    "predictions = model.predict(X_test[:5])\n",
    "print(\"Real values:\", y_test[:5])\n",
    "print(\"Predicted values:\", predictions.reshape(-1))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2. Завдання класифікації (Приклад з Iris)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "# 1. Завантаження набору даних Iris (скористаємося sklearn)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Перетворення міток у one-hot (якщо потрібно)\n",
    "lb = LabelBinarizer()\n",
    "y_onehot = lb.fit_transform(y)\n",
    "\n",
    "# Нормалізація ознак\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Розподіл на тренувальні/тестові дані\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# 2. Створення моделі (MLP)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))  # 3 класи\n",
    "\n",
    "# 3. Компіляція\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Навчання\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=50,\n",
    "                    batch_size=8,\n",
    "                    verbose=1)\n",
    "\n",
    "# 5. Оцінка\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 6. Прогноз\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Predicted classes:\", predicted_classes[:10])\n",
    "print(\"True classes:     \", true_classes[:10])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3. Розпізнавання рукописних цифр (MNIST)\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1. Завантаження даних\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 2. Попередня обробка\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# One-hot для міток\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 3. Створення моделі (CNN)\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 4. Компіляція\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Навчання\n",
    "model.fit(x_train, y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=5, \n",
    "          batch_size=64)\n",
    "\n",
    "# 6. Оцінка\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 7. Приклад передбачення\n",
    "import numpy as np\n",
    "predictions = model.predict(x_test[:5])\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test[:5], axis=1)\n",
    "\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"Real classes:     \", true_classes)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Поради щодо організації групової роботи\n",
    "\n",
    "1. **Робота в малих групах**: слухачів можна розділити на підгрупи, кожна з яких зосередиться на певному типі задач (регресія, класифікація або зображення).  \n",
    "2. **Обговорення результатів**: кожна підгрупа ділиться знахідками, порівнює точність, MAE, метрики класифікації тощо.  \n",
    "3. **Колективна рефлексія**: які кроки дали найкраще поліпшення, який вплив мало збільшення кількості шарів чи зміна оптимізатора і т.д.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5. Підсумки заняття\n",
    "\n",
    "- слухачи ознайомилися з базовою архітектурою нейронних мереж та з практичними прикладами використання `TensorFlow/Keras` для розв’язання задач регресії, класифікації та розпізнавання образів.  \n",
    "- Отримали початковий досвід роботи з популярними наборами даних і розглянули ключові аспекти проектування та тренування глибоких моделей.\n",
    "\n",
    "---\n",
    "\n",
    "**Рекомендовані ресурси для самостійного опрацювання**:\n",
    "- [Офіційна документація TensorFlow](https://www.tensorflow.org/).\n",
    "- Книга “Deep Learning with Python” (François Chollet).\n",
    "- Онлайн-курси з глибокого навчання (Coursera, Udemy, edX).\n",
    "\n",
    "**Завдання додому**:  \n",
    "- Повторити наведені приклади коду, змінити кількість шарів та нейронів, оцінити вплив на точність/помилку.  \n",
    "- Спробувати застосувати CNN-модель на іншому наборі зображень (наприклад, Fashion MNIST).  \n",
    "- Попрацювати з Callbacks (наприклад, EarlyStopping, ModelCheckpoint) для вдосконалення процесу тренування.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижче подано зміст **Розділу 3. Теоретичний блок** з детальними поясненнями та невеликими фрагментами коду, що ілюструють ключові ідеї. Цей блок охоплює два підрозділи: **3.1. Огляд основних методів глибокого навчання** та **3.2. Основи використання методів глибокого навчання для аналізу даних**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Теоретичний блок\n",
    "\n",
    "### 3.1. Огляд основних методів глибокого навчання\n",
    "\n",
    "**Зміст**:\n",
    "\n",
    "1. **Перцептрон** і **багатошарова нейронна мережа (MLP, Multilayer Perceptron)**  \n",
    "   - Базова ідея: дані послідовно проходять через кілька «щільних» (Dense) шарів.  \n",
    "   - Застосування: табличні дані, прості класифікаційні та регресійні задачі, де немає яскраво вираженої просторової чи послідовної структури.\n",
    "\n",
    "2. **Згорткові нейронні мережі (CNN)**  \n",
    "   - Використовуються для обробки зображень та інших даних, що мають просторову структуру (наприклад, 2D-розташування пікселів).  \n",
    "   - Ключова операція – згортка (convolution), яка виділяє локальні ознаки на зображенні.  \n",
    "   - Застосування: розпізнавання образів, класифікація зображень, обробка медичних знімків тощо.\n",
    "\n",
    "3. **Рекурентні нейронні мережі (RNN), LSTM, GRU**  \n",
    "   - Підходять для роботи з послідовностями (текст, часові ряди, аудіодані).  \n",
    "   - LSTM і GRU розв’язують проблему згасаючих градієнтів, що часто виникає у звичайних RNN.  \n",
    "   - Застосування: обробка природної мови, прогнозування часових рядів, генерація послідовностей.\n",
    "\n",
    "4. **Трансформери (Transformers)**  \n",
    "   - Сучасна альтернатива RNN для послідовних даних, яка активно використовується в NLP (Natural Language Processing) і не тільки.  \n",
    "   - Ґрунтується на механізмі «уваги» (attention), що дозволяє моделі вчитися визначати важливі частини входу, не покладаючись на послідовну обробку.  \n",
    "   - Застосування: машинний переклад, генерація текстів (ChatGPT), обробка зображень і мультизадачні моделі.\n",
    "\n",
    "---\n",
    "\n",
    "#### Мініфрагмент коду (порівняльний огляд MLP і CNN)\n",
    "\n",
    "Нижче наведемо короткий приклад для демонстрації **MLP** та **CNN**-архітектур (лише для ілюстрації):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# MLP для табличних даних (наприклад, shape=(None, 10))\n",
    "model_mlp = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(10,)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Припустимо, бінарна класифікація\n",
    "])\n",
    "\n",
    "# CNN для зображень (наприклад, розмір 28х28, 1 канал)\n",
    "model_cnn = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Припустимо, 10 класів\n",
    "])\n",
    "\n",
    "model_mlp.summary()\n",
    "model_cnn.summary()\n",
    "```\n",
    "\n",
    "> **Пояснення**:  \n",
    "> - Перший приклад (MLP) складається з «Dense» шарів, які використовуються для звичайних табличних даних без просторової структури.  \n",
    "> - Другий приклад (CNN) показує, як додати `Conv2D` і `MaxPooling2D` шари для обробки зображень.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2. Основи використання методів глибокого навчання для аналізу даних\n",
    "\n",
    "**Зміст**:\n",
    "\n",
    "1. **Ключові етапи проєкту глибокого навчання**  \n",
    "   1. **Завантаження та попередня обробка даних**  \n",
    "      - Очищення даних, заповнення пропусків, нормалізація чи стандартизація.  \n",
    "      - Підготовка (reshape) для CNN або перетворення послідовностей для RNN.  \n",
    "   2. **Розподіл на тренувальний, валідаційний і тестовий набори**  \n",
    "      - Тренувальний набір використовується для навчання моделі.  \n",
    "      - Валідаційний – для підбору гіперпараметрів (або перевірки пере-/недонавчання).  \n",
    "      - Тестовий – для фінальної оцінки якості.  \n",
    "   3. **Побудова (визначення архітектури) моделі**  \n",
    "      - Вибір типу мережі: MLP, CNN, RNN чи комбінована архітектура.  \n",
    "      - Визначення кількості шарів, кількості нейронів, функцій активації.  \n",
    "   4. **Вибір функції втрат, метрик та оптимізатора**  \n",
    "      - Функція втрат (наприклад, `mse` для регресії, `categorical_crossentropy` для багатокласової класифікації).  \n",
    "      - Метрики (MAE, RMSE, accuracy, F1-score тощо).  \n",
    "      - Оптимізатор (`sgd`, `adam`, `rmsprop` і т.д.).  \n",
    "   5. **Навчання, валідація та оцінка результатів**  \n",
    "      - Параметри навчання: кількість епох, розмір батчу, темп навчання (learning rate).  \n",
    "      - Відстеження метрик на train/val для виявлення можливого пере-/недонавчання.  \n",
    "   6. **Тонке налаштування (Hyperparameter Tuning)**  \n",
    "      - Зміна гіперпараметрів (кількість шарів, нейронів, learning rate, тип оптимізатора, типи шарів і т.п.) з метою покращення результатів.  \n",
    "\n",
    "2. **Основні поняття**:  \n",
    "   - **Епоха (epoch)**: один повний цикл проходу по тренувальному набору даних.  \n",
    "   - **Міні-вибірка (batch)**: підмножина даних, яка використовується для одного кроку оновлення ваг.  \n",
    "   - **Функція активації**: (sigmoid, ReLU, tanh, softmax) – нелінійні перетворення, що додають мережі здатність до складного моделювання.  \n",
    "   - **Регуляризація** (L2, Dropout, BatchNormalization): методи, що допомагають уникнути перенавчання.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Мініфрагмент коду (демонстрація типової послідовності дій)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Припустимо, що ми маємо дані з 1000 зразків і 10 ознак (для класифікації)\n",
    "X_data = np.random.rand(1000, 10).astype(np.float32)\n",
    "y_data = np.random.randint(0, 2, size=(1000,))  # двокласова мітка 0 або 1\n",
    "\n",
    "# 1. Попередня обробка (нормалізація)\n",
    "scaler = StandardScaler()\n",
    "X_data_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "# 2. Розділ на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data_scaled, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Побудова моделі (MLP)\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Бінарна класифікація\n",
    "])\n",
    "\n",
    "# 4. Компіляція\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Навчання (валід. split можна взяти 0.2)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "# 6. Оцінка результатів на тестових даних\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# При бажанні - гіперпараметричний пошук, регуляризація, зміна оптимізатора тощо\n",
    "```\n",
    "\n",
    "> **Пояснення**:  \n",
    "> - Це *мінімальний приклад*, що ілюструє типові кроки. У реальних задачах використовуються розгалуженіші процедури попередньої обробки, складніші архітектури, ретельна перевірка результатів.  \n",
    "> - Суть полягає в тому, щоб підкреслити **основний pipeline**: (1) підготовка даних → (2) розподіл на набори → (3) створення моделі → (4) компіляція → (5) навчання → (6) оцінка.\n",
    "\n",
    "---\n",
    "\n",
    "### Підсумок розділу 3\n",
    "\n",
    "- У **підрозділі 3.1** ми розглянули **основні типи глибоких нейронних мереж** – від класичного MLP до сучасних Трансформерів, звернувши увагу на найбільш поширені галузі застосування кожного методу.  \n",
    "- У **підрозділі 3.2** описали **типовий процес (pipeline) глибокого навчання** та кроки, необхідні для коректного проєктування, навчання та оцінки моделей.  \n",
    "- Короткі **мініфрагменти коду** демонструють, як саме виглядають ці кроки на практиці, що допомагає слухачам краще зрозуміти суть теоретичних концепцій і підготуватися до розгорнутих практичних завдань (розділ «Практичний блок»).\n",
    "\n",
    "Таким чином, теоретичний блок надає міцне підґрунтя для свідомого використання глибоких нейронних мереж у реальних задачах аналізу даних."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижче наведено три приклади розв’язання задач глибокого навчання на різних наборах даних із детальними поясненнями кожного кроку.  \n",
    "\n",
    "1. **Каліфорнійське житло (California Housing)** – **регресія**  \n",
    "2. **IMDB** – **класифікація текстів (сентимент-аналіз)**  \n",
    "3. **Fashion MNIST** – **класифікація зображень**  \n",
    "\n",
    "Усі приклади можна запустити в Google Colab, Jupyter Notebook або іншому середовищі Python із встановленим TensorFlow та (за потреби) scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Регресія на наборі `California Housing`\n",
    "\n",
    "### 1.1. Опис задачі\n",
    "\n",
    "- **Мета**: передбачити середню вартість будинків у певному районі Каліфорнії (середня ціна у тисячах доларів).  \n",
    "- **Джерело**: `sklearn.datasets.fetch_california_housing`.  \n",
    "- **Тип задачі**: регресія (необхідно отримати числове значення).  \n",
    "\n",
    "### 1.2. Покрокове рішення\n",
    "\n",
    "```python\n",
    "# КРОК 0. Імпорт необхідних бібліотек\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# КРОК 1. Завантаження даних\n",
    "data = fetch_california_housing()\n",
    "X = data.data      # Матриця ознак (кількісні показники районів)\n",
    "y = data.target    # Вектор цільової змінної (середня вартість)\n",
    "\n",
    "# Перевіримо розміри\n",
    "print(\"Форма X:\", X.shape)\n",
    "print(\"Форма y:\", y.shape)\n",
    "\n",
    "# КРОК 2. Попередня обробка даних\n",
    "# 2.1. Розділяємо дані на тренувальний і тестовий набори\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2.2. Нормалізація ознак\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# КРОК 3. Побудова моделі\n",
    "# Послідовна модель (MLP) з кількома Dense-шарами\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Вихід: 1 нейрон для регресії\n",
    "\n",
    "# КРОК 4. Компіляція\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='mse',        # Середньоквадратична помилка\n",
    "    metrics=['mae']    # Середня абсолютна похибка\n",
    ")\n",
    "\n",
    "# КРОК 5. Навчання (фітинг)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train, \n",
    "    validation_split=0.2,    # виділяємо 20% від тренувальних даних на валідацію\n",
    "    epochs=20,               # кількість епох\n",
    "    batch_size=32,           # розмір міні-вибірки\n",
    "    verbose=1                # виводимо прогрес навчання\n",
    ")\n",
    "\n",
    "# КРОК 6. Оцінка на тестових даних\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test MSE: {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# КРОК 7. Приклад прогнозу\n",
    "predictions = model.predict(X_test_scaled[:5])\n",
    "print(\"Справжні значення:\", y_test[:5])\n",
    "print(\"Прогнози моделі:   \", predictions.reshape(-1))\n",
    "```\n",
    "\n",
    "### 1.3. Пояснення ключових моментів\n",
    "\n",
    "1. **Завантаження даних**: функція `fetch_california_housing()` повертає `data.data` (матриця ознак) та `data.target` (вартість).  \n",
    "2. **Розділення Train/Test**: використовуємо `train_test_split`, щоб модель «не бачила» тестові дані під час навчання.  \n",
    "3. **Нормалізація**: `StandardScaler` приводить ознаки до нульового середнього та одиничного стандартного відхилення. Це покращує збіжність нейронної мережі.  \n",
    "4. **Архітектура**:  \n",
    "   - Два приховані шари (64 і 32 нейрони).  \n",
    "   - Активація ReLU – стандартний вибір для прихованих шарів.  \n",
    "   - Вихідний шар з 1 нейроном без активації (для регресії).  \n",
    "5. **Функція втрат** – `mse` (Mean Squared Error), метрика – `mae` (Mean Absolute Error).  \n",
    "6. **Оцінка (evaluate)**: отримуємо `MSE` та `MAE` на тестовому наборі.  \n",
    "7. **Прогноз (predict)**: робимо передбачення на кількох перших прикладах, порівнюємо з реальними значеннями.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Класифікація текстів (IMDB)\n",
    "\n",
    "### 2.1. Опис задачі\n",
    "\n",
    "- **Мета**: визначити, чи є відгук про фільм (review) позитивним або негативним.  \n",
    "- **Джерело**: `tf.keras.datasets.imdb`.  \n",
    "- **Тип задачі**: бінарна класифікація (позитив / негатив).  \n",
    "\n",
    "### 2.2. Покрокове рішення\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# КРОК 1. Завантаження даних\n",
    "# num_words=10000 - залишимо 10 000 найбільш вживаних слів\n",
    "num_words = 10000\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n",
    "\n",
    "print(\"Кількість тренувальних прикладів:\", len(x_train))\n",
    "print(\"Приклад тексту (список індексів слів):\", x_train[0])\n",
    "print(\"Мітка (0 - негатив, 1 - позитив):\", y_train[0])\n",
    "\n",
    "# КРОК 2. Попередня обробка\n",
    "# З метою уніфікації довжини рецензій виконуємо паддінг (доповнення) до сталої довжини (наприклад, 200 слів)\n",
    "maxlen = 200\n",
    "x_train_padded = pad_sequences(x_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "x_test_padded  = pad_sequences(x_test,  maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print(\"Форма x_train_padded:\", x_train_padded.shape)\n",
    "print(\"Форма x_test_padded:\", x_test_padded.shape)\n",
    "\n",
    "# КРОК 3. Побудова моделі\n",
    "# Використовуємо Embedding для перетворення індексів слів у вектори,\n",
    "# потім LSTM-шар для вилучення контекстної інформації, і Dense для класифікації\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=64, input_length=maxlen),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# КРОК 4. Компіляція\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# КРОК 5. Навчання\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,       # для демонстрації достатньо 2-3 епох (можна збільшити)\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# КРОК 6. Оцінка на тестових даних\n",
    "test_loss, test_acc = model.evaluate(x_test_padded, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# КРОК 7. Приклад передбачення\n",
    "example_review = x_test[0]  # беремо перший відгук\n",
    "example_review_padded = pad_sequences([example_review], maxlen=maxlen)\n",
    "prediction = model.predict(example_review_padded)[0][0]\n",
    "print(\"Ймовірність позитивного відгуку:\", prediction)\n",
    "print(\"Реальна мітка:\", y_test[0])\n",
    "```\n",
    "\n",
    "### 2.3. Пояснення ключових моментів\n",
    "\n",
    "1. **Завантаження**: `imdb.load_data(num_words=...)` залишає лише найчастіші слова, решту позначає як `oov_token`.  \n",
    "2. **Паддінг**:  \n",
    "   - Різні рецензії мають різну довжину. Для обробки LSTM шаром треба вирівняти їх розмір (наприклад, до 200 токенів).  \n",
    "   - `pad_sequences` додасть нулі в кінець або обріже довгий відгук.  \n",
    "3. **Архітектура**:  \n",
    "   - `Embedding(...)` перетворює індекси слів у вектори розміром `output_dim=64`.  \n",
    "   - `LSTM(64)` – рекурентний шар, що «зчитує» послідовність і зберігає контекст.  \n",
    "   - `Dense(1, activation='sigmoid')` – бінарна класифікація (0 чи 1).  \n",
    "4. **Функція втрат** – `binary_crossentropy`, оскільки маємо бінарні мітки (позитив / негатив).  \n",
    "5. **Оцінка** – `evaluate` дає значення втрати та точність (accuracy) на тесті.  \n",
    "6. **Прогноз** – для прикладу беремо першу рецензію з `x_test`, отримуємо ймовірність «позитиву».\n",
    "\n",
    "> **Примітка**: для кращих результатів можна збільшити `epochs` (наприклад, до 5–10), зробити глибшу архітектуру (додати ще один LSTM/GRU-шар), або використати двонапрямний LSTM (`Bidirectional`).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Класифікація зображень (Fashion MNIST)\n",
    "\n",
    "### 3.1. Опис задачі\n",
    "\n",
    "- **Мета**: класифікувати зображення предметів одягу в 10 категорій (наприклад, футболка, штани, сукня, кросівки тощо).  \n",
    "- **Джерело**: `tf.keras.datasets.fashion_mnist`.  \n",
    "- **Тип задачі**: мультикласова класифікація (10 класів).  \n",
    "\n",
    "### 3.2. Покрокове рішення\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# КРОК 1. Завантаження даних\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(\"Розміри x_train:\", x_train.shape)\n",
    "print(\"Розміри x_test:\", x_test.shape)\n",
    "\n",
    "# КРОК 2. Попередня обробка\n",
    "# Перетворимо [28,28] у [28,28,1] і нормалізуємо до [0..1]\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test  = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# КРОК 3. Побудова моделі (CNN)\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # 10 класів\n",
    "])\n",
    "\n",
    "# КРОК 4. Компіляція\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # оскільки y_train - цілі числа (0..9)\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# КРОК 5. Навчання\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,  # 10% на валідацію\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# КРОК 6. Оцінка\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# КРОК 7. Приклад передбачення\n",
    "import numpy as np\n",
    "predictions = model.predict(x_test[:5])\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"Модель передбачила класи:\", predicted_classes)\n",
    "print(\"Справжні класи:\", y_test[:5])\n",
    "```\n",
    "\n",
    "### 3.3. Пояснення ключових моментів\n",
    "\n",
    "1. **Формат даних**:  \n",
    "   - `x_train`, `y_train`, `x_test`, `y_test` отримуємо з `fashion_mnist.load_data()`.  \n",
    "   - Початково `x_train` має форму `(60000, 28, 28)` – тобто 60 000 прикладів.  \n",
    "2. **Підготовка**:  \n",
    "   - Переформат у 4D: `(кількість, висота, ширина, канали)`. Тут 1 канал (відтінки сірого).  \n",
    "   - Ділимо пікселі на 255.0, щоб привести їх у діапазон [0..1].  \n",
    "3. **Архітектура (CNN)**:  \n",
    "   - Перший `Conv2D(32, (3,3), ...)` шукає 32 фільтри 3×3, MaxPooling2D зменшує просторовий розмір.  \n",
    "   - Другий `Conv2D(64, (3,3), ...)`, знову MaxPooling2D.  \n",
    "   - `Flatten()` перетворює 2D-карту ознак у вектор.  \n",
    "   - `Dense(64, ...)` прихований шар і `Dense(10, softmax)` вихід на 10 класів (0..9).  \n",
    "4. **Функція втрат** – `sparse_categorical_crossentropy`, оскільки мітки (0..9) у вигляді звичайних цілих чисел. Якщо б ми застосували one-hot-кодування, тоді була б `categorical_crossentropy`.  \n",
    "5. **Точність (accuracy)**: показує частку вірних передбачень.  \n",
    "6. **Оцінка** – `evaluate` на тестовому наборі (10 000 зображень).  \n",
    "7. **Прогноз** – `predict(x_test[:5])` дає ймовірності для кожного з 10 класів, `argmax` вибирає найвірогідніший.\n",
    "\n",
    "> **Примітка**: для кращої точності можна збільшити кількість епох і покращити архітектуру (додати більше шарів, використати Dropout тощо).\n",
    "\n",
    "---\n",
    "\n",
    "## Загальний підсумок\n",
    "\n",
    "1. **California Housing** – класичний приклад регресії з використанням MLP.  \n",
    "2. **IMDB** – приклад бінарної класифікації тексту з використанням рекурентних мереж (LSTM).  \n",
    "3. **Fashion MNIST** – приклад мультикласової класифікації зображень за допомогою згорткових нейронних мереж (CNN).\n",
    "\n",
    "Кожен із прикладів демонструє однакову структуру робочого процесу (pipeline) у глибокому навчанні:\n",
    "\n",
    "1. **Завантаження та підготовка даних** (очищення, нормалізація / паддінг, розділення на train/test).  \n",
    "2. **Побудова моделі** (визначення архітектури шарів, функцій активації).  \n",
    "3. **Компіляція** (вибір оптимізатора, функції втрат, метрик).  \n",
    "4. **Навчання** (fit).  \n",
    "5. **Оцінка** (evaluate).  \n",
    "6. **Прогноз** (predict).  \n",
    "\n",
    "Ці кроки є типовими для більшості задач глибокого навчання. Надалі можна експериментувати з гіперпараметрами (кількість шарів, нейронів, тип оптимізатора) або додавати механізми регуляризації (Dropout, BatchNorm) для досягнення кращих результатів."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

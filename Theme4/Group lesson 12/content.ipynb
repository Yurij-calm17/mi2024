{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розширений план групового заняття  \n",
    "**Тема 4. Заняття 12. Використання репозиторіїв аналітичних моделей для аналізу даних (Transfer Learning). Основи використання методів навчання з підкріпленням (Reinforcement Learning).**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Мета заняття**\n",
    "- Ознайомити слухачів із концепцією Transfer Learning та методами використання попередньо навчених моделей з репозиторіїв для аналізу даних.\n",
    "- Ввести основні поняття та принципи Reinforcement Learning (RL), демонструючи їх застосування у військових інформаційно-аналітичних системах.\n",
    "- Забезпечити практичні навички роботи з Transfer Learning та базовими алгоритмами RL через інтерактивні вправи та групові проєкти.\n",
    "\n",
    "### **2. Очікувані результати**\n",
    "Після завершення заняття слухачі зможуть:\n",
    "- Розуміти основні принципи Transfer Learning та його переваги у військовому аналізі даних.\n",
    "- Використовувати попередньо навчені моделі з репозиторіїв (TensorFlow Hub, PyTorch Model Zoo, Hugging Face Model Hub) для вирішення конкретних задач.\n",
    "- Знати основні компоненти та алгоритми Reinforcement Learning.\n",
    "- Реалізовувати прості RL-агенти для вирішення базових задач.\n",
    "- Оцінювати можливості та обмеження Transfer Learning та RL у військових застосуваннях.\n",
    "\n",
    "### **3. Структура заняття**\n",
    "\n",
    "| Час       | Активність                                                                 |\n",
    "|-----------|----------------------------------------------------------------------------|\n",
    "| 0-10 хв   | **Вступ та огляд**<br>- Привітання.<br>- Короткий огляд попередніх занять.<br>- Мета та структура поточного заняття. |\n",
    "| 10-30 хв  | **Transfer Learning: Теоретичний огляд**<br>- Визначення та принципи Transfer Learning.<br>- Переваги використання Transfer Learning у військових задачах.<br>- Репозиторії попередньо навчених моделей (TensorFlow Hub, PyTorch Model Zoo, Hugging Face Model Hub). |\n",
    "| 30-60 хв  | **Практична частина: Використання попередньо навчених моделей**<br>- Демонстрація використання TensorFlow Hub/PyTorch Model Zoo.<br>- Завдання для груп: Вибір моделі та адаптація до конкретної задачі (наприклад, класифікація бойової техніки на зображеннях).<br>- Обговорення результатів. |\n",
    "| 60-70 хв  | **Перерва**                                                              |\n",
    "| 70-90 хв  | **Reinforcement Learning: Теоретичний огляд**<br>- Визначення та основні компоненти RL.<br>- Основні алгоритми: Q-Learning, Deep Q-Network (DQN), Policy Gradients, Actor-Critic.<br>- Застосування RL у військових системах. |\n",
    "| 90-120 хв | **Практична частина: Створення простого RL-агента**<br>- Введення у середовище OpenAI Gym.<br>- Створення та навчання простого агента DQN для гри CartPole.<br>- Обговорення результатів та можливих удосконалень. |\n",
    "| 120-130 хв| **Груповий проєкт**<br>- Розподіл слухачів на групи.<br>- Завдання: Розробити концепцію використання Transfer Learning або RL для конкретної військової задачі.<br>- Обговорення та підготовка презентацій. |\n",
    "| 130-140 хв| **Презентація груп**<br>- Кожна група презентує свою концепцію.<br>- Обговорення та зворотний зв'язок від викладача та інших груп. |\n",
    "| 140-150 хв| **Підсумки та завершення**<br>- Основні висновки заняття.<br>- Відповіді на питання.<br>- Оголошення домашнього завдання (за потреби). |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Детальний навчальний контент**\n",
    "\n",
    "#### **4.1. Transfer Learning: Використання репозиторіїв аналітичних моделей**\n",
    "\n",
    "##### **4.1.1. Теоретичний огляд Transfer Learning**\n",
    "- **Визначення:** Transfer Learning — це метод машинного навчання, при якому знання, набуті при вирішенні однієї задачі, використовуються для вирішення іншої, пов'язаної задачі.\n",
    "- **Принципи:**\n",
    "  - Використання попередньо навчених моделей.\n",
    "  - Адаптація моделей до нових задач шляхом fine-tuning або заміни певних шарів.\n",
    "- **Переваги:**\n",
    "  - Зменшення часу навчання.\n",
    "  - Покращення точності моделей, особливо при обмеженості даних.\n",
    "  - Економія обчислювальних ресурсів.\n",
    "\n",
    "##### **4.1.2. Репозиторії попередньо натренованих моделей**\n",
    "- **TensorFlow Hub:** [https://tfhub.dev/](https://tfhub.dev/)  \n",
    "  Набір моделей для класифікації зображень, обробки тексту тощо.\n",
    "- **PyTorch Model Zoo:** [https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)  \n",
    "  Попередньо навчені моделі для різних задач.\n",
    "- **Hugging Face Model Hub:** [https://huggingface.co/models](https://huggingface.co/models)  \n",
    "  Моделі для обробки природної мови, такі як BERT, GPT, T5.\n",
    "\n",
    "##### **4.1.3. Адаптація моделей під конкретне військове завдання**\n",
    "- **Fine-tuning:** Процес подальшого навчання попередньо натренованої моделі на новому наборі даних.\n",
    "- **Кроки Fine-tuning:**\n",
    "  1. Вибір відповідної попередньо натренованої моделі.\n",
    "  2. Заморожування шарів, що не потребують оновлення.\n",
    "  3. Додавання нових шарів (наприклад, новий вихідний шар для класифікації).\n",
    "  4. Навчання моделі на нових даних з меншою швидкістю навчання.\n",
    "\n",
    "##### **4.1.4. Прикладний кейс: Переднавчена CNN для аналізу зображень бойової техніки**\n",
    "\n",
    "**Задача:** Ідентифікація типу військової техніки на супутникових знімках.\n",
    "\n",
    "**Інструменти:** PyTorch, torchvision, попередньо навчені моделі ResNet.\n",
    "\n",
    "**Приклад коду: Fine-tuning ResNet на новому наборі даних**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# 1. Підготовка даних\n",
    "data_dir = 'path_to_your_dataset'  # Замість цього шляху вкажіть шлях до вашого набору даних\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), \n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                              batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Завантаження попередньо навченого ResNet\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# 3. Заморожування шарів\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 4. Заміна вихідного шару\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 5. Визначення критерію та оптимізатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# 6. Навчання моделі\n",
    "num_epochs = 25\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Кожен епох розділений на тренувальний та валідаційний\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # режим навчання\n",
    "        else:\n",
    "            model.eval()   # режим валідації\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Ітерація по даних\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Прямий прохід\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Зворотний прохід + оптимізація тільки в тренувальному режимі\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Збереження найкращої моделі\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print()\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "# Завантаження найкращої моделі\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Збереження моделі\n",
    "torch.save(model.state_dict(), 'fine_tuned_resnet.pth')\n",
    "```\n",
    "\n",
    "**Пояснення коду:**\n",
    "1. **Підготовка даних:** Використовуються стандартні трансформації для попередньої обробки зображень (Resize, RandomHorizontalFlip, ToTensor, Normalize).\n",
    "2. **Завантаження моделі:** Використовується попередньо навчена модель ResNet50 з бібліотеки torchvision.\n",
    "3. **Заморожування шарів:** Всі шари попередньо навченої моделі заморожуються (`requires_grad = False`), щоб зберегти вже набуті представлення.\n",
    "4. **Заміна вихідного шару:** Вихідний шар змінюється на новий, що відповідає кількості класів у вашому наборі даних.\n",
    "5. **Визначення критерію та оптимізатора:** Використовується CrossEntropyLoss та Adam-оптимізатор для навчання нового шару.\n",
    "6. **Навчання:** Модель навчається лише останнього шару на новому наборі даних. Під час тренувальної фази відбувається оновлення ваг, а під час валідаційної фази модель оцінюється без оновлення ваг.\n",
    "7. **Збереження моделі:** Збереження найкращої моделі на основі валідаційної точності.\n",
    "\n",
    "##### **4.1.5. Приклад застосування Transfer Learning у військовій сфері**\n",
    "\n",
    "**Сценарій:** Використання переднавченої моделі CNN для класифікації типів військової техніки на супутникових знімках.\n",
    "\n",
    "**Кроки:**\n",
    "1. **Збір даних:** Зібрати та анотувати супутникові знімки різних типів військової техніки (танки, БТР, літаки тощо).\n",
    "2. **Fine-tuning:** Використати попередньо навчену модель (наприклад, ResNet50) та адаптувати її до нової задачі, як показано в прикладі коду.\n",
    "3. **Валідація:** Перевірити точність моделі на валідаційному наборі даних, виявити потенційні помилки.\n",
    "4. **Розгортання:** Інтегрувати модель у військову інформаційну систему для автоматизованої класифікації техніки в режимі реального часу.\n",
    "\n",
    "**Очікувані результати:**\n",
    "- **Автоматична класифікація:** Модель здатна автоматично класифікувати техніку на зображеннях, що значно спрощує роботу аналітиків.\n",
    "- **Покращена точність:** Використання переднавченої моделі дозволяє досягти високої точності навіть з обмеженими даними.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2. Основи навчання з підкріпленням (Reinforcement Learning, RL)**\n",
    "\n",
    "##### **4.2.1. Визначення та базові елементи RL**\n",
    "\n",
    "**Reinforcement Learning (RL)** — це підхід у машинному навчанні, де агент навчається приймати рішення шляхом взаємодії зі середовищем для максимізації кумулятивної винагороди.\n",
    "\n",
    "**Основні компоненти RL:**\n",
    "- **Середовище (Environment):** Все, з чим взаємодіє агент (наприклад, гра, симуляція бойових дій).\n",
    "- **Агент (Agent):** Система, яка приймає рішення (наприклад, БПЛА, система логістики).\n",
    "- **Стан (State):** Поточний стан середовища (наприклад, позиція БПЛА, стан ресурсів).\n",
    "- **Дія (Action):** Рішення, яке приймає агент (наприклад, рух у певному напрямку, збір ресурсів).\n",
    "- **Винагорода (Reward):** Зворотний зв'язок від середовища про якість дії агента (наприклад, успішний збір ресурсів, уникнення загрози).\n",
    "\n",
    "##### **4.2.2. Політика, функція винагороди та функція цінності**\n",
    "\n",
    "- **Політика (Policy):** Стратегія агента, яка визначає, яку дію виконати в кожному стані. Позначається як π(a|s).\n",
    "- **Функція винагороди (Reward Function):** Визначає, яку винагороду отримає агент за певну дію в певному стані. Позначається як R(s, a).\n",
    "- **Функція цінності (Value Function):** Визначає очікувану кумулятивну винагороду, яку агент отримає, перебуваючи в певному стані та дотримуючись політики π. Позначається як V^π(s).\n",
    "\n",
    "##### **4.2.3. Сучасні методи RL**\n",
    "\n",
    "- **Q-Learning:** Метод без політики, де агент навчається оцінювати цінність дії в певному стані.\n",
    "- **Deep Q-Network (DQN):** Використовує глибокі нейронні мережі для апроксимації Q-функції.\n",
    "- **Policy Gradients:** Метод, який напряму оптимізує політику агента.\n",
    "- **Actor-Critic:** Комбінує політично-градієнтні методи (Actor) з оцінкою функції цінності (Critic).\n",
    "\n",
    "##### **4.2.4. Приклади застосування RL у військовому контексті**\n",
    "\n",
    "- **Оптимізація логістичних маршрутів:** Навчання агента ефективно переміщувати ресурси в умовах динамічних загроз.\n",
    "- **Автоматизоване керування безпілотними системами:** Навчання агентів приймати рішення в реальних або симульованих бойових умовах.\n",
    "- **Моделювання та планування операцій:** Використання RL для створення оптимальних стратегій у варіативних сценаріях.\n",
    "\n",
    "##### **4.2.5. Приклад коду: Простий агент DQN для гри CartPole**\n",
    "\n",
    "**Інструменти:** Python, PyTorch, OpenAI Gym.\n",
    "\n",
    "**Приклад коду: Реалізація DQN для гри CartPole**\n",
    "\n",
    "```python\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# 1. Визначення нейронної мережі\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=24):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# 2. Ініціалізація середовища\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# 3. Параметри DQN\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "# 4. Ініціалізація моделі та оптимізатора\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(state_size, action_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 5. Функція для вибору дії\n",
    "def act(state):\n",
    "    global epsilon\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 6. Функція для навчання моделі\n",
    "def replay():\n",
    "    global epsilon\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Поточні Q-значення\n",
    "    q_values = model(states).gather(1, actions)\n",
    "    \n",
    "    # Мети Q-значень\n",
    "    next_q_values = model(next_states).max(1)[0].unsqueeze(1)\n",
    "    targets = rewards + (gamma * next_q_values * (1 - dones))\n",
    "    \n",
    "    # Обчислення втрат та зворотний прохід\n",
    "    loss = criterion(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Зменшення epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# 7. Навчання агента\n",
    "num_episodes = 1000\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        replay()\n",
    "    print(f'Episode {e+1}/{num_episodes}, Reward: {total_reward}, Epsilon: {epsilon:.2f}')\n",
    "\n",
    "# 8. Збереження моделі\n",
    "torch.save(model.state_dict(), 'dqn_cartpole.pth')\n",
    "```\n",
    "\n",
    "**Пояснення коду:**\n",
    "1. **Нейронна мережа DQN:** Простий багатошаровий перцептрон для оцінки Q-функції.\n",
    "2. **Середовище:** Використовується OpenAI Gym середовище CartPole.\n",
    "3. **Параметри DQN:** Визначаються основні параметри навчання, такі як розмір батчу, коефіцієнт дисконтування, параметри epsilon.\n",
    "4. **Функція `act`:** Вибір дії на основі політики epsilon-greedy.\n",
    "5. **Функція `replay`:** Навчання моделі на випадкових батчах з пам'яті.\n",
    "6. **Навчання агента:** Проходить через визначену кількість епох, де агент взаємодіє зі середовищем та навчається.\n",
    "7. **Збереження моделі:** Збереження навчених ваг для подальшого використання.\n",
    "\n",
    "##### **4.2.6. Переваги та виклики RL у військових застосуваннях**\n",
    "\n",
    "**Переваги:**\n",
    "- **Адаптивність:** Агент може самостійно навчатися оптимальним стратегіям у динамічному середовищі.\n",
    "- **Ефективність:** Може знаходити нетривіальні рішення, які важко визначити вручну.\n",
    "- **Автоматизація:** Зменшує потребу у ручному плануванні та управлінні складними системами.\n",
    "\n",
    "**Виклики:**\n",
    "- **Обчислювальні ресурси:** Навчання RL-моделей може вимагати значних обчислювальних потужностей.\n",
    "- **Стабільність навчання:** RL-моделі часто нестабільні та важко налаштовуються.\n",
    "- **Безпека:** Потрібно забезпечити, щоб агент не приймав небезпечні дії та був захищений від зовнішніх атак.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Груповий проєкт**\n",
    "\n",
    "#### **5.1. Завдання для груп**\n",
    "- **Група 1:** Розробити концепцію використання Transfer Learning для класифікації типів військової техніки на зображеннях. Описати вибір моделі, підготовку даних, процес fine-tuning та очікувані результати.\n",
    "- **Група 2:** Розробити концепцію використання Reinforcement Learning для управління безпілотним літальним апаратом (БПЛА) у бойових умовах. Описати середовище, цілі агента, вибір алгоритму RL, очікувані переваги та виклики.\n",
    "\n",
    "#### **5.2. Вимоги до проєкту**\n",
    "- **Докладний опис:** Визначення задачі, вибір методів та інструментів, очікувані результати.\n",
    "- **Використання коду:** Інтеграція прикладів коду або розробка власних реалізацій.\n",
    "- **Презентація:** Коротка (5-10 хв) презентація концепції перед класом.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Підсумки та обговорення**\n",
    "\n",
    "- **Основні висновки:**\n",
    "  - Transfer Learning дозволяє ефективно використовувати попередньо навчені моделі для нових задач, зменшуючи час та ресурси, необхідні для навчання.\n",
    "  - Reinforcement Learning надає можливість створювати адаптивних агентів, здатних приймати оптимальні рішення в динамічних середовищах.\n",
    "  - Практичні приклади демонструють реальну застосовність цих методів у військових інформаційно-аналітичних системах.\n",
    "- **Обговорення питань:** Відкрите обговорення викликів та можливостей впровадження Transfer Learning та RL у військові системи.\n",
    "- **Домашнє завдання (за потреби):** Наприклад, реалізувати fine-tuning попередньо навченої моделі на власному наборі даних або створити простого RL-агента для іншої гри чи симуляції.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Рекомендована література та джерела**\n",
    "\n",
    "1. **Ian Goodfellow, Yoshua Bengio, Aaron Courville.** *Deep Learning*. MIT Press, 2016.\n",
    "2. **Francois Chollet.** *Deep Learning with Python*. Manning Publications, 2018.\n",
    "3. **Sutton R.S., Barto A.G.** *Reinforcement Learning: An Introduction*. MIT Press, 2018.\n",
    "4. **Jeremy Howard, Sylvain Gugger.** *Deep Learning for Coders with Fastai and PyTorch*. O'Reilly Media, 2020.\n",
    "5. **TensorFlow Hub:** [https://tfhub.dev/](https://tfhub.dev/)\n",
    "6. **PyTorch Model Zoo:** [https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "7. **Hugging Face Model Hub:** [https://huggingface.co/models](https://huggingface.co/models)\n",
    "8. **OpenAI Gym:** [https://gym.openai.com/](https://gym.openai.com/)\n",
    "9. **Матеріали конференцій MILCOM, ICLR, ICML, NeurIPS:** сучасні дослідження з машинного та глибокого навчання, зокрема у військовому та безпековому контексті.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Додаткові ресурси та матеріали**\n",
    "\n",
    "- **Відео-лекції та туторіали:**\n",
    "  - YouTube-канали, такі як **deeplizard**, **sentdex**, **PyTorch**, **TensorFlow**, які пропонують детальні відео-уроки з Transfer Learning та Reinforcement Learning.\n",
    "- **Онлайн-курси:**\n",
    "  - **Coursera:** *Deep Learning Specialization* від Andrew Ng.\n",
    "  - **edX:** *Reinforcement Learning* від Microsoft.\n",
    "  - **Udemy:** Курси з Transfer Learning та RL.\n",
    "- **Документація та туторіали:**\n",
    "  - **PyTorch Tutorials:** [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)\n",
    "  - **TensorFlow Tutorials:** [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)\n",
    "  - **Hugging Face Tutorials:** [https://huggingface.co/transformers/tutorials.html](https://huggingface.co/transformers/tutorials.html)\n",
    "  - **OpenAI Gym Documentation:** [https://gym.openai.com/docs/](https://gym.openai.com/docs/)\n",
    "\n",
    "---\n",
    "\n",
    "**Примітка:** Для ефективного засвоєння матеріалу рекомендується активна участь у практичних завданнях, обговореннях та групових проєктах. Викладач може адаптувати складність завдань залежно від рівня підготовки слухачів та доступних ресурсів.\n",
    "\n",
    "---\n",
    "\n",
    "**Завершення:** Це заняття націлене на поглиблення розуміння слухачами сучасних методів машинного навчання та їх застосування у військовій сфері, забезпечуючи необхідні теоретичні знання та практичні навички для ефективного використання Transfer Learning та Reinforcement Learning у реальних інформаційно-аналітичних задачах."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = [' '] * 9\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return ''.join(self.board)\n",
    "\n",
    "    def available_actions(self):\n",
    "        return [i for i, v in enumerate(self.board) if v == ' ']\n",
    "\n",
    "    def make_move(self, action, player):\n",
    "        if self.board[action] == ' ':\n",
    "            self.board[action] = player\n",
    "            self.check_game_over()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def check_game_over(self):\n",
    "        win_combos = [(0,1,2),(3,4,5),(6,7,8),\n",
    "                      (0,3,6),(1,4,7),(2,5,8),\n",
    "                      (0,4,8),(2,4,6)]\n",
    "        for a, b, c in win_combos:\n",
    "            if self.board[a] == self.board[b] == self.board[c] != ' ':\n",
    "                self.winner = self.board[a]\n",
    "                self.done = True\n",
    "                return\n",
    "        if ' ' not in self.board:\n",
    "            self.done = True\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, player, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
    "        self.player = player\n",
    "        self.q_table = {}  # state -> action values\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def choose_action(self, state, actions):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(actions)\n",
    "        q_values = self.q_table.get(state, np.zeros(9))\n",
    "        best_action = max(actions, key=lambda x: q_values[x])\n",
    "        return best_action\n",
    "\n",
    "    def update(self, state, reward):\n",
    "        if self.last_state is not None:\n",
    "            q_values = self.q_table.setdefault(self.last_state, np.zeros(9))\n",
    "            next_q = self.q_table.get(state, np.zeros(9))\n",
    "            q_values[self.last_action] += self.alpha * (reward + self.gamma * max(next_q) - q_values[self.last_action])\n",
    "\n",
    "    def remember(self, state, action):\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "\n",
    "def train(episodes=10000):\n",
    "    env = TicTacToe()\n",
    "    agent_X = QAgent('X')\n",
    "    agent_O = QAgent('O')\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        current_player = agent_X\n",
    "        other_player = agent_O\n",
    "        while not env.done:\n",
    "            actions = env.available_actions()\n",
    "            action = current_player.choose_action(state, actions)\n",
    "            env.make_move(action, current_player.player)\n",
    "            next_state = env.get_state()\n",
    "            if env.done:\n",
    "                if env.winner == current_player.player:\n",
    "                    current_player.update(next_state, 1)\n",
    "                    other_player.update(next_state, -1)\n",
    "                elif env.winner is None:\n",
    "                    current_player.update(next_state, 0.5)\n",
    "                    other_player.update(next_state, 0.5)\n",
    "                break\n",
    "            current_player.remember(state, action)\n",
    "            current_player.update(next_state, 0)\n",
    "            state = next_state\n",
    "            current_player, other_player = other_player, current_player\n",
    "    return agent_X\n",
    "\n",
    "def play(agent):\n",
    "    env = TicTacToe()\n",
    "    state = env.reset()\n",
    "    while not env.done:\n",
    "        print(\"Current board:\")\n",
    "        print_board(env.board)\n",
    "        if env.board.count('X') <= env.board.count('O'):\n",
    "            actions = env.available_actions()\n",
    "            action = agent.choose_action(state, actions)\n",
    "            env.make_move(action, 'X')\n",
    "        else:\n",
    "            action = int(input(\"Your move (0-8): \"))\n",
    "            env.make_move(action, 'O')\n",
    "        state = env.get_state()\n",
    "\n",
    "    print(\"Game Over!\")\n",
    "    print_board(env.board)\n",
    "    if env.winner:\n",
    "        print(f\"Winner: {env.winner}\")\n",
    "    else:\n",
    "        print(\"Draw!\")\n",
    "\n",
    "def print_board(board):\n",
    "    print(f\"{board[0]} | {board[1]} | {board[2]}\")\n",
    "    print(\"-\" * 5)\n",
    "    print(f\"{board[3]} | {board[4]} | {board[5]}\")\n",
    "    print(\"-\" * 5)\n",
    "    print(f\"{board[6]} | {board[7]} | {board[8]}\")\n",
    "\n",
    "# Example usage:\n",
    "trained_agent = train(1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current board:\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "Current board:\n",
      "X |   |  \n",
      "-----\n",
      "  |   |  \n",
      "-----\n",
      "  |   |  \n",
      "Current board:\n",
      "X |   |  \n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  |   |  \n",
      "Current board:\n",
      "X |   | X\n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  |   |  \n",
      "Current board:\n",
      "X | O | X\n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  |   |  \n",
      "Current board:\n",
      "X | O | X\n",
      "-----\n",
      "  | O |  \n",
      "-----\n",
      "  | X |  \n",
      "Current board:\n",
      "X | O | X\n",
      "-----\n",
      "O | O |  \n",
      "-----\n",
      "  | X |  \n",
      "Current board:\n",
      "X | O | X\n",
      "-----\n",
      "O | O | X\n",
      "-----\n",
      "  | X |  \n",
      "Current board:\n",
      "X | O | X\n",
      "-----\n",
      "O | O | X\n",
      "-----\n",
      "  | X |  \n",
      "Current board:\n",
      "X | O | X\n",
      "-----\n",
      "O | O | X\n",
      "-----\n",
      "  | X | O\n",
      "Game Over!\n",
      "X | O | X\n",
      "-----\n",
      "O | O | X\n",
      "-----\n",
      "X | X | O\n",
      "Draw!\n"
     ]
    }
   ],
   "source": [
    "play(trained_agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

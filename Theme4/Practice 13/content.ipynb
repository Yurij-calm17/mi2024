{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Розширений план практичного заняття  \n",
    "**Тема 4. Заняття 13. Практичне використання репозиторіїв аналітичних моделей в межах виконання індивідуальних (групових) проектів**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Мета заняття**\n",
    "- Забезпечити слухачам практичні навички роботи з репозиторіями попередньо навчених моделей (Transfer Learning) та методами Reinforcement Learning (RL).\n",
    "- Навчити слухачів ефективно використовувати попередньо навчені моделі для вирішення конкретних інформаційно-аналітичних завдань у військовій сфері.\n",
    "- Сприяти розвитку командної роботи та співпраці через виконання групових проектів.\n",
    "\n",
    "### **2. Очікувані результати**\n",
    "Після завершення заняття слухачі зможуть:\n",
    "- Використовувати репозиторії аналітичних моделей для вирішення індивідуальних завдань.\n",
    "- Співпрацювати в групах для розробки комплексних рішень, використовуючи попередньо навчені моделі та методи RL.\n",
    "- Презентувати та обговорювати результати своїх проектів, отримуючи зворотний зв'язок для подальшого вдосконалення.\n",
    "\n",
    "### **3. Структура заняття**\n",
    "\n",
    "| Час        | Активність                                                                                   |\n",
    "|------------|----------------------------------------------------------------------------------------------|\n",
    "| 0-10 хв    | **Вступ та огляд**<br>- Привітання.<br>- Огляд цілей та структури заняття.                 |\n",
    "| 10-30 хв   | **Індивідуальна робота: Використання репозиторіїв аналітичних моделей**<br>- Інструктаж та пояснення завдань.<br>- Демонстрація прикладів використання моделей з TensorFlow Hub/PyTorch Model Zoo. |\n",
    "| 30-60 хв   | **Практична частина: Fine-Tuning попередньо навчених моделей**<br>- Завдання для слухачів: вибрати модель та адаптувати її до конкретного завдання (наприклад, класифікація зображень військової техніки).<br>- Надання кодових прикладів та підтримка викладача. |\n",
    "| 60-70 хв   | **Перерва**                                                                                   |\n",
    "| 70-90 хв   | **Групова робота: Використання RL у військових сценаріях**<br>- Формування груп.<br>- Визначення завдань для груп: розробка RL-агента для симуляції бойової ситуації або оптимізації логістичних маршрутів. |\n",
    "| 90-120 хв  | **Практична частина: Розробка RL-агентів**<br>- Надання прикладів коду (DQN, Actor-Critic).<br>- Групове програмування та інтеграція моделей у симульоване середовище. |\n",
    "| 120-130 хв | **Презентація групових проектів**<br>- Кожна група презентує свою концепцію та результати.<br>- Обговорення та зворотний зв'язок від викладача та інших груп. |\n",
    "| 130-140 хв | **Обговорення та аналіз**<br>- Висвітлення успіхів та труднощів.<br>- Рекомендації щодо покращення проектів. |\n",
    "| 140-150 хв | **Підсумки та завершення**<br>- Основні висновки заняття.<br>- Оголошення домашнього завдання (за потреби). |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Детальний навчальний контент**\n",
    "\n",
    "#### **4.1. Індивідуальна робота: Використання репозиторіїв аналітичних моделей**\n",
    "\n",
    "##### **4.1.1. Інструктаж та пояснення завдань**\n",
    "- **Завдання:** Кожен слухач обирає одну з попередньо навчених моделей з TensorFlow Hub або PyTorch Model Zoo для вирішення конкретної задачі (наприклад, класифікація зображень військової техніки, аналіз текстових донесень).\n",
    "- **Мета:** Навчитися адаптувати попередньо навчену модель до власного набору даних через fine-tuning.\n",
    "\n",
    "##### **4.1.2. Демонстрація прикладів використання моделей**\n",
    "\n",
    "**Приклад: Fine-Tuning ResNet50 для класифікації військової техніки (PyTorch)**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# 1. Підготовка даних\n",
    "data_dir = 'path_to_your_dataset'  # Вкажіть шлях до вашого набору даних\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), \n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                              batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Завантаження попередньо навченого ResNet\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# 3. Заморожування шарів\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 4. Заміна вихідного шару\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 5. Визначення критерію та оптимізатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# 6. Навчання моделі\n",
    "num_epochs = 25\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Кожен епох розділений на тренувальний та валідаційний\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # режим навчання\n",
    "        else:\n",
    "            model.eval()   # режим валідації\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Ітерація по даних\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Прямий прохід\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Зворотний прохід + оптимізація тільки в тренувальному режимі\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "        # Збереження найкращої моделі\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print()\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "# Завантаження найкращої моделі\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Збереження моделі\n",
    "torch.save(model.state_dict(), 'fine_tuned_resnet.pth')\n",
    "```\n",
    "\n",
    "**Пояснення коду:**\n",
    "1. **Підготовка даних:** Використовуються трансформації для попередньої обробки зображень (Resize, RandomHorizontalFlip, ToTensor, Normalize).\n",
    "2. **Завантаження моделі:** Використовується попередньо навчена модель ResNet50 з бібліотеки torchvision.\n",
    "3. **Заморожування шарів:** Всі шари попередньо навченої моделі заморожуються (`requires_grad = False`), щоб зберегти вже набуті представлення.\n",
    "4. **Заміна вихідного шару:** Вихідний шар змінюється на новий, що відповідає кількості класів у вашому наборі даних.\n",
    "5. **Визначення критерію та оптимізатора:** Використовується CrossEntropyLoss та Adam-оптимізатор для навчання нового шару.\n",
    "6. **Навчання:** Модель навчається лише останнього шару на новому наборі даних. Під час тренувальної фази відбувається оновлення ваг, а під час валідаційної фази модель оцінюється без оновлення ваг.\n",
    "7. **Збереження моделі:** Збереження найкращої моделі на основі валідаційної точності.\n",
    "\n",
    "##### **4.1.3. Завдання для слухачів**\n",
    "\n",
    "**Завдання:**\n",
    "1. **Вибір моделі:** Оберіть попередньо навчену модель з TensorFlow Hub або PyTorch Model Zoo, яка найбільше підходить для вашої задачі.\n",
    "2. **Адаптація моделі:** Виконайте fine-tuning моделі на вашому наборі даних.\n",
    "3. **Оцінка результатів:** Перевірте точність моделі на валідаційних даних та оптимізуйте параметри при необхідності.\n",
    "4. **Презентація результатів:** Підготуйте коротку презентацію з описом вашої задачі, вибору моделі, процесу адаптації та отриманих результатів.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4.2. Групова робота: Використання RL у військових сценаріях**\n",
    "\n",
    "##### **4.2.1. Інструктаж та пояснення завдань**\n",
    "\n",
    "**Завдання:** Кожна група обирає військову інформаційно-аналітичну задачу, для якої застосовуватиме методи Reinforcement Learning. Наприклад, розробка RL-агента для оптимізації логістичних маршрутів або управління безпілотними літальними апаратами (БПЛА) у симульованих бойових умовах.\n",
    "\n",
    "**Мета:** Розробити концепцію та реалізувати простий RL-агент, який може вирішувати обрану задачу в симульованому середовищі.\n",
    "\n",
    "##### **4.2.2. Приклад практичної частини: Створення RL-агента для оптимізації логістичних маршрутів**\n",
    "\n",
    "**Задача:** Оптимізація маршруту доставки ресурсів у змінних умовах бойової обстановки.\n",
    "\n",
    "**Інструменти:** Python, PyTorch, OpenAI Gym (створення власного середовища або використання існуючих середовищ).\n",
    "\n",
    "**Приклад коду: Реалізація простого RL-агента за допомогою DQN**\n",
    "\n",
    "```python\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# 1. Визначення нейронної мережі\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=24):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# 2. Ініціалізація середовища\n",
    "env = gym.make('CartPole-v1')  # Для групового проекту рекомендується створити власне середовище\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# 3. Параметри DQN\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "# 4. Ініціалізація моделі та оптимізатора\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(state_size, action_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 5. Функція для вибору дії\n",
    "def act(state):\n",
    "    global epsilon\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 6. Функція для навчання моделі\n",
    "def replay():\n",
    "    global epsilon\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Поточні Q-значення\n",
    "    q_values = model(states).gather(1, actions)\n",
    "    \n",
    "    # Мети Q-значень\n",
    "    next_q_values = model(next_states).max(1)[0].unsqueeze(1)\n",
    "    targets = rewards + (gamma * next_q_values * (1 - dones))\n",
    "    \n",
    "    # Обчислення втрат та зворотний прохід\n",
    "    loss = criterion(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Зменшення epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# 7. Навчання агента\n",
    "num_episodes = 1000\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        replay()\n",
    "    print(f'Episode {e+1}/{num_episodes}, Reward: {total_reward}, Epsilon: {epsilon:.2f}')\n",
    "\n",
    "# 8. Збереження моделі\n",
    "torch.save(model.state_dict(), 'dqn_cartpole.pth')\n",
    "```\n",
    "\n",
    "**Пояснення коду:**\n",
    "1. **Нейронна мережа DQN:** Простий багатошаровий перцептрон для оцінки Q-функції.\n",
    "2. **Середовище:** Використовується OpenAI Gym середовище CartPole для демонстрації. У групових проектах рекомендується створити власне середовище, що відображає військовий сценарій.\n",
    "3. **Параметри DQN:** Визначаються основні параметри навчання, такі як розмір батчу, коефіцієнт дисконтування, параметри epsilon.\n",
    "4. **Функція `act`:** Вибір дії на основі політики epsilon-greedy.\n",
    "5. **Функція `replay`:** Навчання моделі на випадкових батчах з пам'яті.\n",
    "6. **Навчання агента:** Проходить через визначену кількість епох, де агент взаємодіє зі середовищем та навчається.\n",
    "7. **Збереження моделі:** Збереження навчених ваг для подальшого використання.\n",
    "\n",
    "##### **4.2.3. Завдання для груп**\n",
    "\n",
    "**Завдання:**\n",
    "1. **Вибір сценарію:** Оберіть військову інформаційно-аналітичну задачу, для якої застосовуватиметься Reinforcement Learning. Наприклад:\n",
    "   - Оптимізація логістичних маршрутів доставки ресурсів.\n",
    "   - Управління безпілотними літальними апаратами (БПЛА) у бойових умовах.\n",
    "   - Автоматизоване планування та виконання бойових операцій у змінних сценаріях.\n",
    "2. **Розробка середовища:** Створіть або адаптуйте симульоване середовище, яке відображає обрану задачу.\n",
    "3. **Розробка агента:** Виберіть алгоритм RL (наприклад, DQN, Actor-Critic) та реалізуйте агента, здатного вирішувати обрану задачу.\n",
    "4. **Навчання та тестування:** Навчіть агента у створеному середовищі та оцініть його продуктивність.\n",
    "5. **Презентація:** Підготуйте коротку презентацію з описом завдання, розробленого рішення та отриманих результатів.\n",
    "\n",
    "##### **4.2.4. Практична частина: Розробка RL-агента**\n",
    "\n",
    "**Приклад: Використання DQN для оптимізації логістичних маршрутів**\n",
    "\n",
    "**Сценарій:** Агент повинен навчитися вибирати оптимальний маршрут доставки ресурсів між двома точками на карті з урахуванням змінних умов (наприклад, наявність загроз, обмеження на час).\n",
    "\n",
    "**Кроки:**\n",
    "1. **Створення середовища:** Використання бібліотеки OpenAI Gym для створення власного середовища або модифікація існуючого.\n",
    "2. **Розробка та навчання агента:** Використання методу DQN для навчання агента вибору маршруту.\n",
    "3. **Оцінка:** Тестування агента на різних сценаріях та аналіз його продуктивності.\n",
    "\n",
    "**Приклад коду: Створення власного середовища**\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class LogisticsEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Просте середовище для оптимізації логістичних маршрутів.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LogisticsEnv, self).__init__()\n",
    "        # Простір дій: переміщення в 4 напрямках\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Простір станів: координати (x, y) на карті 10x10\n",
    "        self.observation_space = spaces.Box(low=0, high=9, shape=(2,), dtype=np.int32)\n",
    "        # Початкова позиція та ціль\n",
    "        self.start = np.array([0, 0])\n",
    "        self.goal = np.array([9, 9])\n",
    "        self.state = self.start.copy()\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start.copy()\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            return self.state, 0, self.done, {}\n",
    "        \n",
    "        # Визначення нової позиції\n",
    "        if action == 0:   # Вгору\n",
    "            self.state[1] = min(self.state[1] + 1, 9)\n",
    "        elif action == 1: # Вниз\n",
    "            self.state[1] = max(self.state[1] - 1, 0)\n",
    "        elif action == 2: # Вліво\n",
    "            self.state[0] = max(self.state[0] - 1, 0)\n",
    "        elif action == 3: # Вправо\n",
    "            self.state[0] = min(self.state[0] + 1, 9)\n",
    "        \n",
    "        # Перевірка досягнення цілі\n",
    "        if np.array_equal(self.state, self.goal):\n",
    "            reward = 10\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = -1  # Негативна винагорода за кожен крок\n",
    "        \n",
    "        return self.state, reward, self.done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        grid = np.zeros((10, 10), dtype=str)\n",
    "        grid[:] = '.'\n",
    "        grid[self.goal[1], self.goal[0]] = 'G'\n",
    "        grid[self.state[1], self.state[0]] = 'A'\n",
    "        print('\\n'.join([' '.join(row) for row in grid]))\n",
    "```\n",
    "\n",
    "**Приклад коду: Навчання агента у створеному середовищі**\n",
    "\n",
    "```python\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# 1. Визначення нейронної мережі\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=24):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# 2. Ініціалізація середовища\n",
    "env = LogisticsEnv()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# 3. Параметри DQN\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "memory = deque(maxlen=5000)\n",
    "\n",
    "# 4. Ініціалізація моделі та оптимізатора\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN(state_size, action_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 5. Функція для вибору дії\n",
    "def act(state):\n",
    "    global epsilon\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state)\n",
    "    return torch.argmax(q_values).item()\n",
    "\n",
    "# 6. Функція для навчання моделі\n",
    "def replay():\n",
    "    global epsilon\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "    \n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Поточні Q-значення\n",
    "    q_values = model(states).gather(1, actions)\n",
    "    \n",
    "    # Мети Q-значень\n",
    "    next_q_values = model(next_states).max(1)[0].unsqueeze(1)\n",
    "    targets = rewards + (gamma * next_q_values * (1 - dones))\n",
    "    \n",
    "    # Обчислення втрат та зворотний прохід\n",
    "    loss = criterion(q_values, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Зменшення epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# 7. Навчання агента\n",
    "num_episodes = 500\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        replay()\n",
    "    print(f'Episode {e+1}/{num_episodes}, Reward: {total_reward}, Epsilon: {epsilon:.2f}')\n",
    "\n",
    "# 8. Збереження моделі\n",
    "torch.save(model.state_dict(), 'dqn_logistics.pth')\n",
    "```\n",
    "\n",
    "**Пояснення коду:**\n",
    "1. **Створення середовища:** Використовується власне середовище `LogisticsEnv`, яке симулює задачу оптимізації маршрутів.\n",
    "2. **Нейронна мережа DQN:** Модель оцінює Q-функцію для вибору дій агента.\n",
    "3. **Параметри DQN:** Визначаються основні параметри навчання, включаючи розмір батчу, коефіцієнт дисконтування та параметри epsilon.\n",
    "4. **Функція `act`:** Вибір дії за політикою epsilon-greedy.\n",
    "5. **Функція `replay`:** Навчання моделі на випадкових батчах з пам'яті.\n",
    "6. **Навчання агента:** Агент навчається проходити через серії епізодів у середовищі, оптимізуючи свою стратегію.\n",
    "7. **Збереження моделі:** Збереження навчених ваг для подальшого використання або аналізу.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Груповий проєкт**\n",
    "\n",
    "#### **5.1. Завдання для груп**\n",
    "\n",
    "**Група 1:** **Transfer Learning для класифікації бойової техніки**\n",
    "- **Задача:** Розробити модель для класифікації типів військової техніки на зображеннях супутникових знімків.\n",
    "- **Кроки:**\n",
    "  1. Вибір попередньо навченої моделі (наприклад, ResNet50).\n",
    "  2. Збір та анотування набору даних зображень різних типів техніки.\n",
    "  3. Адаптація моделі через fine-tuning.\n",
    "  4. Оцінка та тестування моделі.\n",
    "  5. Презентація результатів.\n",
    "\n",
    "**Група 2:** **Reinforcement Learning для управління БПЛА**\n",
    "- **Задача:** Створити RL-агента для оптимізації маршрутів безпілотного літального апарата (БПЛА) у бойових умовах.\n",
    "- **Кроки:**\n",
    "  1. Визначення середовища симуляції (наприклад, використання OpenAI Gym або створення власного середовища).\n",
    "  2. Розробка та навчання RL-агента (DQN, Actor-Critic).\n",
    "  3. Тестування агента у різних сценаріях.\n",
    "  4. Аналіз продуктивності та оптимізація.\n",
    "  5. Презентація результатів.\n",
    "\n",
    "#### **5.2. Вимоги до проєкту**\n",
    "- **Докладний опис:** Чітко визначена задача, вибір методів та інструментів, опис процесу розробки та навчання моделі.\n",
    "- **Використання коду:** Інтеграція прикладів коду або розробка власних реалізацій, адаптованих до задачі.\n",
    "- **Презентація:** Коротка (5-10 хв) презентація перед класом з демонстрацією результатів та обговоренням викликів.\n",
    "- **Документація:** Підготовка документації з описом кроків розробки, використаних технологій та отриманих результатів.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Підсумки та обговорення**\n",
    "\n",
    "- **Основні висновки:**\n",
    "  - **Transfer Learning** дозволяє ефективно використовувати попередньо навчені моделі для нових задач, що значно знижує час та ресурси на навчання.\n",
    "  - **Reinforcement Learning** надає можливість створювати адаптивних агентів, здатних приймати оптимальні рішення в складних та змінних середовищах.\n",
    "  - **Практичні завдання** демонструють реальну застосовність методів Transfer Learning та RL у військових інформаційно-аналітичних системах.\n",
    "  \n",
    "- **Обговорення питань:**\n",
    "  - Які були основні труднощі при адаптації моделей?\n",
    "  - Як можна покращити точність моделей?\n",
    "  - Які додаткові методи або технології можна інтегрувати для покращення інформаційно-аналітичного забезпечення?\n",
    "  \n",
    "- **Домашнє завдання (за потреби):**\n",
    "  - Інтегрувати навчену модель Transfer Learning у симульовану інформаційну систему.\n",
    "  - Розробити та навчити більш складного RL-агента для обраної задачі, враховуючи додаткові параметри або умови.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Рекомендована література та джерела**\n",
    "\n",
    "1. **Ian Goodfellow, Yoshua Bengio, Aaron Courville.** *Deep Learning*. MIT Press, 2016.\n",
    "2. **Francois Chollet.** *Deep Learning with Python*. Manning Publications, 2018.\n",
    "3. **Sutton R.S., Barto A.G.** *Reinforcement Learning: An Introduction*. MIT Press, 2018.\n",
    "4. **Jeremy Howard, Sylvain Gugger.** *Deep Learning for Coders with Fastai and PyTorch*. O'Reilly Media, 2020.\n",
    "5. **TensorFlow Hub:** [https://tfhub.dev/](https://tfhub.dev/)\n",
    "6. **PyTorch Model Zoo:** [https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)\n",
    "7. **Hugging Face Model Hub:** [https://huggingface.co/models](https://huggingface.co/models)\n",
    "8. **OpenAI Gym:** [https://gym.openai.com/](https://gym.openai.com/)\n",
    "9. **Документація PyTorch:** [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)\n",
    "10. **Документація TensorFlow:** [https://www.tensorflow.org/learn](https://www.tensorflow.org/learn)\n",
    "11. **Матеріали конференцій MILCOM, ICLR, ICML, NeurIPS:** сучасні дослідження з машинного та глибокого навчання, зокрема у військовому та безпековому контексті.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Додаткові ресурси та матеріали**\n",
    "\n",
    "- **Відео-лекції та туторіали:**\n",
    "  - YouTube-канали, такі як **deeplizard**, **sentdex**, **PyTorch**, **TensorFlow**, які пропонують детальні відео-уроки з Transfer Learning та Reinforcement Learning.\n",
    "- **Онлайн-курси:**\n",
    "  - **Coursera:** *Deep Learning Specialization* від Andrew Ng.\n",
    "  - **edX:** *Reinforcement Learning* від Microsoft.\n",
    "  - **Udemy:** Курси з Transfer Learning та RL.\n",
    "- **Документація та туторіали:**\n",
    "  - **PyTorch Tutorials:** [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)\n",
    "  - **TensorFlow Tutorials:** [https://www.tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)\n",
    "  - **Hugging Face Tutorials:** [https://huggingface.co/transformers/tutorials.html](https://huggingface.co/transformers/tutorials.html)\n",
    "  - **OpenAI Gym Documentation:** [https://gym.openai.com/docs/](https://gym.openai.com/docs/)\n",
    "\n",
    "---\n",
    "\n",
    "**Примітка:** Для ефективного засвоєння матеріалу рекомендується активна участь у практичних завданнях, обговореннях та групових проєктах. Викладач може адаптувати складність завдань залежно від рівня підготовки слухачів та доступних ресурсів.\n",
    "\n",
    "---\n",
    "\n",
    "**Завершення:** Це практичне заняття спрямоване на закріплення теоретичних знань з Transfer Learning та Reinforcement Learning через практичну роботу з попередньо навченими моделями та розробку RL-агентів. Слухачі отримають цінний досвід у використанні сучасних методів машинного навчання для вирішення конкретних військових інформаційно-аналітичних задач, що сприятиме їх професійному розвитку та підвищенню компетентності у сфері забезпечення національної безпеки."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

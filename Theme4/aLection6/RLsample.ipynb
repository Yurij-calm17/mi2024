{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-таблиця після навчання:\n",
      "[[[ 2.12533649e+00  2.38003364e+00  3.02347806e+00  4.78296900e+00]\n",
      "  [ 3.67410947e+00  5.31441000e+00  2.19886201e+00  2.62129631e+00]\n",
      "  [ 0.00000000e+00  1.41675512e-02  4.62199490e+00  9.67548693e-03]\n",
      "  [ 0.00000000e+00  0.00000000e+00  1.52725632e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  7.49604744e-02  0.00000000e+00]]\n",
      "\n",
      " [[ 4.27803080e+00  3.82136996e-01  3.09841743e-01  5.31440767e-01]\n",
      "  [ 3.69446683e+00  5.90490000e+00  2.23710807e+00  8.54378866e-01]\n",
      "  [ 2.58368073e+00 -4.13320961e+00  0.00000000e+00  6.92418015e-05]\n",
      "  [ 2.99929402e-01  1.96143870e-03  0.00000000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 3.42982133e+00  3.21019081e-02  4.61222696e-01  5.90490000e-01]\n",
      "  [ 4.09480236e+00  6.56100000e+00  1.67969348e+00 -4.92758250e+00]\n",
      "  [ 2.00394709e-01  7.25408790e+00  5.90427203e-01  1.09083979e-03]\n",
      "  [ 4.58411960e-02  0.00000000e+00 -8.41102871e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 1.73496040e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 3.86832024e+00  4.13798455e+00  5.84691972e-01  7.29000000e+00]\n",
      "  [-3.78706171e+00  8.10000000e+00  4.60611843e+00  4.09662132e+00]\n",
      "  [ 4.34229188e-03  8.11370619e+00  1.38510000e+00  0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      " [[ 2.81643112e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 6.39269206e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  [ 4.81084935e+00  5.09205952e+00  4.17018646e+00  9.00000000e+00]\n",
      "  [ 3.50631453e+00  6.45027229e+00  6.24638646e+00  1.00000000e+01]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n",
      "Оптимальний шлях:\n",
      "[(0, 0), (0, 1), (1, 1), (2, 1), (3, 1), (3, 2), (4, 2), (4, 3), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Параметри середовища\n",
    "grid_size = 5  # Розмір території (5x5)\n",
    "num_actions = 4  # Дії: вгору, вниз, ліворуч, праворуч\n",
    "gamma = 0.9  # Дисконтуючий фактор\n",
    "alpha = 0.1  # Швидкість навчання\n",
    "epsilon = 0.1  # Імовірність випадкового вибору дії (ε-жадібна стратегія)\n",
    "num_episodes = 500  # Кількість епізодів навчання\n",
    "\n",
    "# Створення таблиці Q\n",
    "Q = np.zeros((grid_size, grid_size, num_actions))\n",
    "\n",
    "# Функції для вибору дій і оновлення стану\n",
    "def choose_action(state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)  # Випадкова дія\n",
    "    return np.argmax(Q[state])  # Жадібна дія\n",
    "\n",
    "def take_action(state, action):\n",
    "    x, y = state\n",
    "    if action == 0:  # Вгору\n",
    "        x = max(0, x - 1)\n",
    "    elif action == 1:  # Вниз\n",
    "        x = min(grid_size - 1, x + 1)\n",
    "    elif action == 2:  # Ліворуч\n",
    "        y = max(0, y - 1)\n",
    "    elif action == 3:  # Праворуч\n",
    "        y = min(grid_size - 1, y + 1)\n",
    "    return (x, y)\n",
    "\n",
    "# Нагороди: -1 за кожен крок, +10 за досягнення цілі, -10 за \"небезпечну\" зону\n",
    "rewards = np.zeros((grid_size, grid_size))\n",
    "rewards[4, 4] = 10  # Ціль\n",
    "rewards[2, 2] = -10  # Небезпечна зона\n",
    "\n",
    "# Навчання агента\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Початкове положення\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state = take_action(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        done = next_state == (4, 4)  # Завершення, якщо досягнуто цілі\n",
    "        \n",
    "        # Оновлення Q-таблиці\n",
    "        Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
    "        state = next_state\n",
    "\n",
    "# Перевірка результатів\n",
    "print(\"Q-таблиця після навчання:\")\n",
    "print(Q)\n",
    "\n",
    "# Тестування оптимальної стратегії\n",
    "state = (0, 0)\n",
    "path = [state]\n",
    "while state != (4, 4):\n",
    "    action = np.argmax(Q[state])\n",
    "    state = take_action(state, action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Оптимальний шлях:\")\n",
    "print(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
